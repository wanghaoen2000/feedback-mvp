# 任务书：排查AI生成内容截断问题

## 问题描述

小班课学情反馈生成时，内容在约 5881 字符（约 8000 token 左右）处被截断，finish_reason 返回 `length` 或 `max_tokens`。但我们在请求体中明确传了 `max_tokens: 64000`。

以前（V64版及之前），同样的代码路径能一口气生成 2-3 万字，不存在截断问题。现在突然只能输出约 5000-8000 字就被截了。

## 当前API调用方式

我们通过 OpenAI 兼容格式调用，代码如下：

```typescript
// 文件: server/whatai.ts - invokeWhatAIStream 函数
const response = await fetch(`${baseUrl}/chat/completions`, {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    "Authorization": `Bearer ${apiKey}`,
  },
  body: JSON.stringify({
    model,          // "claude-sonnet-4-5-20250929"
    messages,       // [{ role: "system", content: ... }, { role: "user", content: ... }]
    max_tokens,     // 64000
    temperature,    // 0.7
    stream: true,   // 流式输出
  }),
});
```

- **API地址**: `https://www.DMXapi.com/v1/chat/completions`
- **模型**: `claude-sonnet-4-5-20250929`
- **max_tokens**: `64000`
- **stream**: `true`（流式）

## 需要排查的问题

1. **DMXapi 代理是否有输出 token 上限？**
   - 是否有 per-request 的输出 token 限制（比如 8192）？
   - 这个限制是否和 API key 的套餐等级有关？
   - `stream: true` 和 `stream: false` 的输出限制是否不同？

2. **max_tokens 参数是否被正确传递给上游？**
   - DMXapi 是否会将 `max_tokens: 64000` 原样传给 Claude API？
   - 还是会被 clamp 到某个上限值？
   - 是否需要用 `max_completion_tokens` 而非 `max_tokens`？（OpenAI 新版 API 改用了这个参数名）

3. **V64版本时期（2026年1月24日前后）为什么能正常生成长内容？**
   - 当时使用的模型是否不同？（比如 claude-3-5-sonnet-20240620）
   - 当时 DMXapi 的输出限制是否更高？
   - 是否有什么配置或参数在那之后发生了变化？

4. **Claude Sonnet 4.5 的实际最大输出 token 数是多少？**
   - `claude-sonnet-4-5-20250929` 原生最大输出是 16384 还是更多？
   - DMXapi 代理是否支持将 max_tokens 设到 16384 以上？

## 期望结果

我们需要能单次生成 2-3 万字符（约 10000-15000 token）的内容。请告诉我们：
- 需要怎么调整 API 调用参数才能实现？
- 如果是 DMXapi 的限制，有没有办法解除或提升？
- 如果必须分多次生成，最佳实践是什么？

## 补充信息

- 生成内容是中文为主的学情反馈文档
- System prompt（路书）长度约 3000-10000 字符
- User prompt（含录音转文字、课堂笔记等）长度约 5000-30000 字符
- 期望输出长度：10000-30000 字符
